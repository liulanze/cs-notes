# Tokenization

## Prefill / Decoding

### 1

"ä»€ä¹ˆæ˜¯Prefillå’ŒDecodingï¼ŸPrefillæ¨¡å‹ä¼šä¸€æ¬¡æ€§å¤„ç†ä½ è¾“å…¥çš„æ•´ä¸ªpromptï¼Œç†è§£å®ƒçš„æ„æ€ï¼Œå¹¶ç”Ÿæˆä¸€ä¸ªåˆå§‹çš„ä¸Šä¸‹æ–‡è¡¨ç¤ºï¼Œä¸ºåé¢çš„ç”Ÿæˆåšå¥½é“ºå«ã€‚Decodingæ˜¯ç”Ÿæˆé˜¶æ®µï¼Œæ¨¡å‹ä¼šåŸºäºprefillé˜¶æ®µçš„ä¸Šä¸‹æ–‡ï¼Œé€æ­¥ç”Ÿæˆè¾“å‡ºçš„æ¯ä¸ªè¯ï¼ˆæˆ–å«tokenï¼‰ï¼Œç›´åˆ°å®Œæˆæ•´ä¸ªå›ç­”ã€‚"

What are Prefill and Decoding? A prefill pass processes your entire input prompt at once, understands its meaning, and builds an initial contextual representation to set up the generation. Decoding is the generation phase: based on the prefill context, the model produces the output token by token until the full answer is complete.

### 2

â€œä¸ºä»€ä¹ˆè¦æœ‰Prefillå’ŒDecodeï¼Ÿï¼ˆ1ï¼‰LLMæ˜¯è‡ªå›å½’æ¨¡å‹ï¼Œæ„æ€æ˜¯ä»–ç”Ÿæˆæ–‡æœ¬æ—¶ç±»ä¼¼äºæ¥é¾™ä¸€æ ·ï¼Œæ¯ç”Ÿæˆä¸€ä¸ªè¯ï¼Œéƒ½ä¼šæŠŠè¿™ä¸ªè¯åŠ åˆ°ä¸Šä¸‹æ–‡ä¸­ï¼Œå†å»é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ã€‚æ¯”å¦‚ç”Ÿæˆä»Šå¤©å¤©æ°”å¾ˆå¥½æ—¶ï¼Œå…ˆç”Ÿæˆä»Šå¤©ï¼Œç„¶åæ ¹æ®ä»Šå¤©ç”Ÿäº§å¤©æ°”ï¼Œå†æ ¹æ®ä»Šå¤©å¤©æ°”ç”Ÿæˆå¾ˆå¥½ã€‚è¿™ç§æ–¹å¼ä¿è¯æ–‡æœ¬è¿è´¯ï¼Œæœ‰é€»è¾‘ï¼Œä½†ä¹Ÿæ„å‘³ç€å¿…é¡»ä¸€æ­¥æ­¥æ¥ã€‚ ï¼ˆ2ï¼‰å¦‚æœæ¨¡å‹ä¸€æ¬¡æ€§ç”Ÿæˆæ‰€æœ‰è¯ï¼Œæ¯”å¦‚é¢„æµ‹æ•´å¥è¯ï¼Œè®¡ç®—ä¼šå˜å¾—éå¸¸å¤æ‚ï¼Œå› ä¸ºå®ƒéœ€è¦åŒæ—¶è€ƒè™‘æ‰€æœ‰å¯èƒ½çš„è¯ç»„åˆï¼Œæ¶ˆè€—çš„ç®—åŠ›å’Œæ—¶é—´ä¼šå¤§å¹…å¢åŠ ã€‚è€Œåˆ†æˆprefillå’Œdecodeä¸¤ä¸ªé˜¶æ®µï¼Œprefillå…ˆè¾“å…¥ï¼Œdecodeåœ¨é€æ­¥ç”Ÿæˆï¼Œèƒ½æŠŠä»»åŠ¡åˆ†æˆå°å—ï¼Œæ›´é«˜æ•ˆã€‚(3)æ¯ä¸ªç”Ÿæˆçš„è¯éƒ½ä¾èµ–äºå‰é¢çš„è¯ï¼Œæ¯”å¦‚å¤©æ°”åé¢æ¥å¾ˆå¥½æ¯”æ¥è·‘æ­¥æ›´åˆç†ï¼Œè¿™ç§ä¾èµ–æ€§è¦æ±‚æ¨¡å‹é€æ­¥ç”Ÿæˆï¼Œè€Œä¸æ˜¯è·³è·ƒæ€§é¢„æµ‹ã€‚â€

Why have Prefill and Decode? (1) LLMs are autoregressive: generation works like a chainâ€”after generating a word, itâ€™s appended to the context to predict the next one. For example, to produce â€œä»Šå¤©å¤©æ°”å¾ˆå¥½,â€ it first generates â€œä»Šå¤©,â€ then, based on â€œä»Šå¤©,â€ generates â€œå¤©æ°”,â€ and then, based on â€œä»Šå¤©å¤©æ°”,â€ generates â€œå¾ˆå¥½.â€ This ensures coherence and logic, but it also means generation must proceed step by step. (2) If the model tried to generate everything at once (e.g., an entire sentence), computation would become very complex because it would need to consider all possible combinations simultaneously, greatly increasing compute and time. Splitting into prefill (ingesting the input) and decode (stepwise generation) breaks the task into smaller pieces and is more efficient. (3) Each generated token depends on prior tokensâ€”for instance, after â€œå¤©æ°”,â€ â€œå¾ˆå¥½â€ is more reasonable than â€œè·‘æ­¥.â€ This dependency requires incremental generation rather than jumping straight to the final output.

### 3

"Prefillé˜¶æ®µå…·ä½“åšäº†ä»€ä¹ˆï¼Ÿå‡è®¾ä½ è¾“å…¥çš„promptæ˜¯ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Œprefillä¼šåšè¿™äº›äº‹ï¼šï¼ˆ1ï¼‰æ¨¡å‹ä¼šå…ˆæŠŠä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·æ‹†æˆä¸€ä¸ªä¸ªå°å•å…ƒï¼ˆtokenï¼‰ï¼Œæ¯”å¦‚ä»Šå¤©ï¼Œå¤©æ°”ï¼Œæ€ä¹ˆæ ·ã€‚ç„¶åæŠŠè¿™äº›tokenè½¬æˆæ•°å­—è¡¨ç¤ºï¼ˆåµŒå…¥å‘é‡ï¼Œembeddingï¼‰ï¼Œtokenizationã€‚ï¼ˆ2ï¼‰LLMç”¨çš„æ˜¯Transformeræ¶æ„ï¼Œé‡Œé¢æœ‰ä¸ªæ ¸å¿ƒæœºåˆ¶å«è‡ªæ³¨æ„åŠ›ï¼ˆself-attentionï¼‰ã€‚åœ¨prefillé˜¶æ®µï¼Œæ¨¡å‹ä¼šåˆ†æprompté‡Œæ¯ä¸ªtokenå’Œå…¶ä»–tokençš„å…³ç³»ï¼Œæ¯”å¦‚æ€ä¹ˆæ ·è·Ÿå¤©æ°”å…³ç³»æ›´å¯†åˆ‡ï¼Œé€šè¿‡è¿™ç§è®¡ç®—ï¼Œæ¨¡å‹ä¼šç”Ÿæˆä¸€ä¸ªåŒ…å«å…¨å±€ä¸Šä¸‹æ–‡çš„è¡¨ç¤ºï¼Œç†è§£æ•´ä¸ªå¥å­çš„æ„æ€ã€‚ï¼ˆ3ï¼‰åœ¨æ³¨æ„åŠ›è®¡ç®—ä¸­ï¼Œæ¨¡å‹ä¼šä¸ºæ¯ä¸ªtokenç”ŸæˆKey/Value matrixï¼Œè¿™äº›å‘é‡è®°å½•äº†æ¯ä¸ªè¯åœ¨ä¸Šä¸‹æ–‡contextä¸­çš„ä¿¡æ¯ã€‚è¿™äº›keyå’Œvalueä¼šè¢«å­˜åˆ°KV cacheä¸­ï¼Œåé¢decodeé˜¶æ®µä¼šç›´æ¥ç”¨ä»–ä»¬ï¼Œçœçš„é‡å¤è®¡ç®—ã€‚"

What does the Prefill phase actually do?
Suppose your input prompt is â€œä»Šå¤©å¤©æ°”æ€ä¹ˆæ · (Howâ€™s the weather today).â€ The prefill phase performs the following steps:
(1) The model first breaks down the prompt into small units called tokensâ€”for example: ä»Šå¤© (today), å¤©æ°” (weather), æ€ä¹ˆæ · (how). Then, it converts these tokens into numerical representations known as embeddings (this process is called tokenization).
(2) LLMs use the Transformer architecture, whose core mechanism is self-attention. During the prefill stage, the model analyzes the relationship between each token and every other token in the promptâ€”for instance, recognizing that â€œæ€ä¹ˆæ ·â€ is more closely related to â€œå¤©æ°”.â€ Through this computation, the model builds a global contextual representation that captures the overall meaning of the entire sentence.
(3) In this attention computation, the model generates Key and Value matrices for each token. These vectors record contextual information for each word within the sentence. The keys and values are then stored in the KV cache, which the model will directly reuse during the decoding phaseâ€”saving time and avoiding redundant calculations.

ä¸ºä»€ä¹ˆ Prefill é˜¶æ®µåªç¼“å­˜ K/Vï¼Œè€Œä¸æ˜¯ Qï¼Ÿ

å› ä¸ºåœ¨è§£ç é˜¶æ®µï¼ˆdecodingï¼‰ï¼Œæˆ‘ä»¬è¦ä¸€ä¸ªä¸€ä¸ªåœ°ç”Ÿæˆæ–°çš„ tokenã€‚

æ¯ç”Ÿæˆä¸€ä¸ªæ–°çš„ tokenï¼Œå®ƒä¼šæœ‰æ–°çš„ Qï¼ˆqueryï¼‰å»æŸ¥è¯¢ä¹‹å‰æ‰€æœ‰ token çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

è€Œä¹‹å‰çš„ K/V ä¸å˜ã€å¯ä»¥å¤ç”¨ã€‚
æ‰€ä»¥æˆ‘ä»¬åªéœ€è¦ç¼“å­˜ K/Vï¼Œ
æ¯æ¬¡ decode æ—¶è®¡ç®—æ–°çš„ Qï¼Œå»å’Œç¼“å­˜çš„ K/V åš attentionã€‚

ğŸ” æ€»ç»“ä¸€å¥è¯ï¼š

Prefill é˜¶æ®µï¼šç”Ÿæˆ**æ‰€æœ‰**tokençš„ Q/K/Vï¼Œå…¨é‡è®¡ç®— self-attentionï¼›åŒæ—¶ç¼“å­˜ K/Vã€‚initial Qçš„è®¡ç®—ç»“æœé€šè¿‡ self-attention å¾—åˆ°çš„è¯­ä¹‰å…³ç³»ï¼Œä¼šè¢«æ•´åˆè¿› hidden statesï¼ˆéšå±‚è¡¨ç¤ºï¼‰ã€‚

Decoding é˜¶æ®µï¼šåªä¸ºå½“å‰æ–° token è®¡ç®— Qï¼Œå¤ç”¨ç¼“å­˜çš„ K/Vï¼Œè¿›è¡Œå¢é‡ self-attentionã€‚

### 4

â€œDecodeé˜¶æ®µå…·ä½“åšäº†ä»€ä¹ˆï¼Ÿï¼ˆ1ï¼‰ ä»–ä¼šå…ˆé¢„æµ‹ä¸‹ä¸€ä¸ªtokenï¼ŒåŸºäºprefillç”Ÿæˆçš„ä¸Šä¸‹æ–‡ï¼Œæ¨¡å‹é¢„æµ‹ç¬¬ä¸€ä¸ªtokençš„æ¦‚ç‡åˆ†å¸ƒï¼Œæ¯”å¦‚å¾ˆï¼Œä¸é”™ï¼Œæ™´æœ—ï¼Œç­‰ç­‰ï¼Œç„¶ååˆ©ç”¨ç®—æ³•é€‰ä¸€ä¸ªï¼ˆæ¯”å¦‚å¾ˆï¼‰ ï¼ˆ2ï¼‰ æ›´æ–°ä¸Šä¸‹æ–‡ï¼ŒæŠŠå¾ˆåŠ åˆ°ä¸Šä¸‹æ–‡é‡Œï¼Œç°åœ¨ä¸Šä¸‹æ–‡å˜æˆä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·å¾ˆï¼ŒåŒæ—¶ï¼Œè®¡ç®—å¾ˆçš„key/valueï¼Œæ›´æ–°åˆ°kv cacheä¸­ã€‚ï¼ˆ3ï¼‰é‡å¤æ­¥éª¤ï¼ŒåŸºäºæœ€æ–°ä¸Šä¸‹æ–‡ï¼Œç»§ç»­é¢„æµ‹ä¸‹ä¸€ä¸ªtokenï¼ŒçŸ¥é“ç”Ÿæˆç»“æŸç¬¦æˆ–è¾¾åˆ°æœ€å¤§é•¿åº¦ã€‚â€

What does the Decode phase actually do?
(1) It first predicts the next token. Based on the context built during the prefill phase, the model estimates a probability distribution over possible next tokensâ€”such as â€œå¾ˆ (very),â€ â€œä¸é”™ (nice),â€ or â€œæ™´æœ— (clear)â€â€”and then selects one using a sampling algorithm (for example, choosing â€œå¾ˆâ€).
(2) The context is then updated: the token â€œå¾ˆâ€ is appended to the existing context, so the new context becomes â€œä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·å¾ˆ.â€ At the same time, the model computes the key/value representations for this new token and adds them to the KV cache.
(3) The model repeats this processâ€”using the updated context to predict the next tokenâ€”until it reaches an end-of-sequence token or the predefined maximum generation length.

åœ¨ decoding é˜¶æ®µï¼Œå½“æ–° tokenï¼ˆæ¯”å¦‚â€œå¾ˆâ€ï¼‰è¢«ç”Ÿæˆæ—¶ï¼š

æ¨¡å‹ä¼š è®¡ç®—è¿™ä¸ªæ–° token çš„ Q/K/Vï¼›

æ—§ token çš„ K/V ä¸ä¼šé‡æ–°è®¡ç®—ï¼›

æ—§ token çš„ Q ä¹Ÿä¸ä¼šé‡æ–°è®¡ç®—ã€‚

ä¹Ÿå°±æ˜¯è¯´ï¼Œåªæœ‰æ–° token çš„ Q ä¼šå˜åŒ–ï¼ˆå› ä¸ºå®ƒæ˜¯æ–°ç”Ÿæˆçš„ token æ‰éœ€è¦å‘èµ·æŸ¥è¯¢ï¼‰ï¼Œè€Œä¹‹å‰çš„ token çš„ Q/K/V éƒ½ä¿æŒä¸å˜ã€‚

### 5

Tokenization / detokenization happens on CPU usually, no matmul or parallel computing.
